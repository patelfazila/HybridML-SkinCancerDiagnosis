{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "wOZ-pwF5sbpm",
        "outputId": "f9ece4e2-73f1-4e69-fbc8-3ee1a7e8dd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2647 images belonging to 2 classes.\n",
            "Found 660 images belonging to 2 classes.\n",
            "CNN feature\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "finish CNN\n",
            "feature extract\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22s/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1ca7d398e017>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Extract features from the train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature extract\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1ca7d398e017>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(generator, model)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbatch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define directories for train and test data\n",
        "train_dir = '/content/drive/MyDrive/Small_skin/train'\n",
        "test_dir = '/content/drive/MyDrive/Small_skin/test'\n",
        "\n",
        "# Image parameters\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data generators for loading .bmp images\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load the VGG16 model without the top layer (include_top=False)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "cnn_model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "\n",
        "# Function to extract features using the CNN\n",
        "def extract_features(generator, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for batch in generator:\n",
        "        imgs, lbls = batch\n",
        "        batch_features = model.predict(imgs)\n",
        "        features.append(batch_features)\n",
        "        labels.extend(lbls)\n",
        "        if len(features) * generator.batch_size >= generator.samples:\n",
        "            break\n",
        "    features = np.vstack(features)\n",
        "    labels = np.array(labels)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features from the train and test sets\n",
        "train_features, train_labels = extract_features(train_generator, cnn_model)\n",
        "test_features, test_labels = extract_features(test_generator, cnn_model)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "# Train SVM and Gradient Boosting classifiers\n",
        "svm_classifier = SVC(kernel='linear', probability=True)\n",
        "svm_classifier.fit(train_features, train_labels)\n",
        "\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\n",
        "gb_classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Function to combine predictions\n",
        "def combine_predictions(svm_pred, gb_pred):\n",
        "    combined_pred = np.zeros_like(svm_pred)\n",
        "    for i in range(len(svm_pred)):\n",
        "        if svm_pred[i] == gb_pred[i]:\n",
        "            combined_pred[i] = svm_pred[i]\n",
        "        else:\n",
        "            combined_pred[i] = gb_pred[i]\n",
        "    return combined_pred\n",
        "\n",
        "# Make predictions on test set\n",
        "test_svm_pred = svm_classifier.predict(test_features)\n",
        "test_gb_pred = gb_classifier.predict(test_features)\n",
        "test_combined_pred = combine_predictions(test_svm_pred, test_gb_pred)\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "test_accuracy = accuracy_score(test_labels, test_combined_pred)\n",
        "print('Test Accuracy:', test_accuracy * 100)\n",
        "print(classification_report(test_labels, test_combined_pred))\n",
        "\n",
        "# Confusion matrix for test set\n",
        "test_cm = confusion_matrix(test_labels, test_combined_pred)\n",
        "\n",
        "# Plot confusion matrix as heatmap\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_generator.class_indices.keys(), yticklabels=train_generator.class_indices.keys())\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, test_combined_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(test_labels, test_combined_pred, average='weighted')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(test_labels, test_combined_pred, average='weighted')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(test_labels, test_combined_pred, average='weighted')\n",
        "print(f'F1 score: {f1:.2f}')\n",
        "\n",
        "# Calculate specificity\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "print(f'Specificity: {specificity:.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}